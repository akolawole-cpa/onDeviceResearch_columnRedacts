{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f3b4ace-0321-469b-b3ac-cd6882d9ed2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from eda.feature_engineering import (\n",
    "    create_task_amount_features,\n",
    "    create_task_temporal_features,\n",
    "    create_task_speed_features,\n",
    "    create_respondent_behavioral_features,\n",
    "    add_wonky_features,\n",
    "    create_fraud_risk_score,\n",
    "    create_wonky_risk_score,\n",
    "    add_rating_delta\n",
    ")\n",
    "from eda.statistical_tests import (\n",
    "    perform_chi_square_tests,\n",
    "    perform_mannwhitney_tests,\n",
    "    perform_welch_ttests,\n",
    "    perform_two_proportion_z_tests,\n",
    "    perform_welch_ttests_on_proportions,\n",
    ")\n",
    "\n",
    "from eda.visualizations import (\n",
    "    create_breakdown_summary,\n",
    "    create_breakdown_chart,\n",
    "    create_task_speed_breakdown_summary,\n",
    "    create_chi_squared_bar_chart,\n",
    "    create_dual_axis_statistical_chart,\n",
    "    create_feature_breakdown_table,\n",
    "    create_distribution_comparison,\n",
    "    calculate_temporal_feature_deltas,       \n",
    "    create_chi_squared_delta_dual_axis_chart,\n",
    ")\n",
    "\n",
    "# Load configs\n",
    "with open('../configs/feature_engineering.yaml', 'r') as f:\n",
    "    feature_config = yaml.safe_load(f)\n",
    "\n",
    "with open('../configs/data_paths.yaml', 'r') as f:\n",
    "    paths_config = yaml.safe_load(f)\n",
    "\n",
    "with open('../configs/statistical_tests.yaml', 'r') as f:\n",
    "    stats_config = yaml.safe_load(f)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"✓ Imports and configs loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb0bf37f-5c67-4239-899e-6af75d237c4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "notebook_path = os.getcwd() \n",
    "repo_root = os.path.abspath(os.path.join(notebook_path, \"..\"))\n",
    "misc_dir = os.path.join(repo_root, \"misc\")\n",
    "\n",
    "output_path = os.path.join(misc_dir,\n",
    "                           os.path.basename(paths_config['output_files']['user_info_df_post_eda']))\n",
    "wonky_counts_path = os.path.join(misc_dir,\n",
    "                          os.path.basename(paths_config['output_files']['wonky_user_counts']))\n",
    "wonky_respondent_df_path = os.path.join(misc_dir,\n",
    "                          os.path.basename(paths_config['output_files']['wonky_respondent_df']))\n",
    "wonky_respondent_summary_path = os.path.join(misc_dir,\n",
    "                          os.path.basename(paths_config['output_files']['wonky_respondent_summary']))\n",
    "\n",
    "user_info_df = pd.read_parquet(output_path) # total user info\n",
    "wonky_counts = pd.read_parquet(wonky_counts_path) # normal tasks and wonky tasks for wonky task respondents|\n",
    "wonky_respondent_df = pd.read_parquet(wonky_respondent_df_path) # task level info for wonky task respondents\n",
    "wonky_respondent_summary = pd.read_parquet(wonky_respondent_summary_path) # summary of wonky task respondents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1729170f-8599-460c-b046-f88544a65622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def aggregate_user_level_proportions(\n",
    "    df: pd.DataFrame,\n",
    "    feature_set: list,\n",
    "    group_var: str = \"wonky_study_count\",\n",
    "    user_id_var: str = \"respondentPk\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate user x task data to user-level proportions.\n",
    "    \n",
    "    For each user, calculates the proportion of tasks where each binary feature = 1.\n",
    "    This is the KEY step that restores independence.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        User x task level data\n",
    "    feature_set : List[str]\n",
    "        Binary features to aggregate\n",
    "    group_var : str\n",
    "        Group indicator (wonky_study_count)\n",
    "    user_id_var : str\n",
    "        User identifier column\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        One row per user with their proportion for each feature\n",
    "    \"\"\"\n",
    "    # Prepare group variable\n",
    "    df_prep = df.copy()\n",
    "    df_prep[group_var] = df_prep[group_var].fillna(0)\n",
    "    df_prep[\"group_binary\"] = (df_prep[group_var] > 0).astype(int)\n",
    "    \n",
    "    # Build list of columns to aggregate\n",
    "    cols_to_aggregate = ['group_binary']\n",
    "    for feature in feature_set:\n",
    "        if feature in df_prep.columns:\n",
    "            cols_to_aggregate.append(feature)\n",
    "    \n",
    "    # Select only needed columns\n",
    "    df_subset = df_prep[[user_id_var] + cols_to_aggregate].copy()\n",
    "    \n",
    "    # Aggregate: mean for proportions\n",
    "    user_df = df_subset.groupby(user_id_var, as_index=False).mean()\n",
    "    \n",
    "    # Add task count separately\n",
    "    task_counts = df_prep.groupby(user_id_var, as_index=False).size()\n",
    "    task_counts.columns = [user_id_var, 'n_tasks']\n",
    "    \n",
    "    # Merge\n",
    "    user_df = user_df.merge(task_counts, on=user_id_var, how='left')\n",
    "    \n",
    "    return user_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da9efffe-a0ff-4e80-98e8-441ede0ebcb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_set = stats_config['feature_sets']['temporal_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f261f50f-de7f-40b6-a7fc-07f81c3a561c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_df = aggregate_user_level_proportions(\n",
    "    df=user_info_df,\n",
    "    feature_set=feature_set\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e75106-9970-48cf-a47d-48fee51c9bd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1316d742-abb2-4a17-a044-00fd75635867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test the fixed function\n",
    "print(\"Testing aggregation function...\")\n",
    "\n",
    "user_df = aggregate_user_level_proportions(\n",
    "    df=user_info_df,\n",
    "    feature_set=feature_set,\n",
    "    group_var='wonky_study_count',\n",
    "    user_id_var='respondentPk'\n",
    ")\n",
    "\n",
    "print(f\"\\nOriginal data: {len(user_info_df)} rows (user x task)\")\n",
    "print(f\"Aggregated data: {len(user_df)} rows (unique users)\")\n",
    "print(f\"\\nFirst few users:\")\n",
    "print(user_df.head())\n",
    "\n",
    "print(f\"\\nTask count distribution:\")\n",
    "print(user_df['n_tasks'].describe())\n",
    "\n",
    "print(f\"\\nGroup distribution:\")\n",
    "print(user_df['group_binary'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8c3de8a-b95a-4fb1-a34e-af960e0c3870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "respondent_eda_user_level_aggregation_analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
