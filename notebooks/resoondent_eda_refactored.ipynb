{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "566aaab3-ec7f-4b46-b960-716fb14053ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pyyaml>=6.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7566b84-4277-4e34-8a4a-c7c8ea000116",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import yaml with error handling\n",
    "try:\n",
    "    import yaml\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"PyYAML is not installed. Please run the previous cell to install it, \"\n",
    "        \"or run: %pip install pyyaml>=6.0\"\n",
    "    )\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Import our modular functions\n",
    "from eda.feature_engineering import (\n",
    "    create_time_features,\n",
    "    create_task_speed_features,\n",
    "    create_respondent_behavioral_features,\n",
    "    add_wonky_features,\n",
    "    create_fraud_risk_score,\n",
    "    create_wonky_risk_score\n",
    ")\n",
    "from eda.statistical_tests import (\n",
    "    compare_groups_statistically,\n",
    "    compare_groups_with_both_tests,\n",
    "    analyze_thresholds,\n",
    "    perform_chi_square_tests,\n",
    "    compare_demographic_groups\n",
    ")\n",
    "from eda.visualizations import (\n",
    "    create_histogram,\n",
    "    create_box_plot,\n",
    "    create_scatter_plot,\n",
    "    create_bar_plot,\n",
    "    create_temporal_breakdown_summary,\n",
    "    create_chi_squared_bar_chart,\n",
    "    create_dual_axis_statistical_chart,\n",
    "    create_feature_breakdown_table,\n",
    "    create_distribution_comparison\n",
    ")\n",
    "\n",
    "# Load configuration files\n",
    "with open('../configs/feature_engineering.yaml', 'r') as f:\n",
    "    feature_config = yaml.safe_load(f)\n",
    "\n",
    "with open('../configs/statistical_tests.yaml', 'r') as f:\n",
    "    stats_config = yaml.safe_load(f)\n",
    "\n",
    "with open('../configs/data_paths.yaml', 'r') as f:\n",
    "    paths_config = yaml.safe_load(f)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"✓ Imports and configs loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1c38d56-93e9-4cb5-9bc2-ef786af50c36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### File Definitions\n",
    "\n",
    "- **user_info_df**: DataFrame of respondent x task level data for all users (not just wonky studies)\n",
    "- **wonky_studies_df**: DataFrame of respondents involved in studies with unexpected outcomes (negative impacts when positive expected)\n",
    "\n",
    "A study is \"wonky\" if the outcome is unexpected (e.g., advertisement showed negative impacts of media, which is counter-intuitive).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16f52c87-8b48-473a-9e72-fd6ffc9eff72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b39bb0f-203b-49c3-a86d-9d1ef0d84005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load data files\n",
    "# Why: Loads processed data from previous data pull notebook\n",
    "# Files are saved in misc folder to keep them out of git\n",
    "\n",
    "notebook_path = os.getcwd() \n",
    "repo_root = os.path.abspath(os.path.join(notebook_path, \"..\"))\n",
    "misc_dir = os.path.join(repo_root, \"misc\")\n",
    "os.makedirs(misc_dir, exist_ok=True)\n",
    "\n",
    "# Handle both commented and uncommented output_files in config\n",
    "if 'output_files' in paths_config and paths_config['output_files']:\n",
    "    # Use paths from config if available\n",
    "    output_path = os.path.join(misc_dir,\n",
    "                               os.path.basename(paths_config['output_files']['user_info_df']))\n",
    "    wonky_path = os.path.join(misc_dir,\n",
    "                              os.path.basename(paths_config['output_files']['wonky_user_counts']))\n",
    "else:\n",
    "    # Fallback to default filenames\n",
    "    output_path = os.path.join(misc_dir, \"user_info_df_pullcomplete.parquet\")\n",
    "    wonky_path = os.path.join(misc_dir, \"wonky_user_counts_pullcomplete.parquet\")\n",
    "\n",
    "user_info_df = pd.read_parquet(output_path)\n",
    "wonky_studies_df = pd.read_parquet(wonky_path)\n",
    "\n",
    "print(\"Files loaded successfully:\")\n",
    "print(f\"  - {output_path}\")\n",
    "print(f\"  - {wonky_path}\")\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"  - user_info_df: {user_info_df.shape}\")\n",
    "print(f\"  - wonky_studies_df: {wonky_studies_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc086073-cde3-4547-8c26-d7dffaa84ab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb5cb33e-6d9d-44e6-a1ca-f3e0078d7899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create time features using modular function\n",
    "user_info_df = create_time_features(user_info_df, date_col=\"date_completed\")\n",
    "\n",
    "print(f\"Night tasks: {user_info_df['is_night'].mean()*100:.1f}%\")\n",
    "print(f\"Weekend tasks: {user_info_df['is_weekend'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Feature Analysis & Breakdowns\n",
    "\n",
    "Analyzing temporal patterns to identify differences between wonky and non-wonky study tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal feature breakdown summary\n",
    "# Why: Shows percentage breakdowns for temporal features across all tasks, wonky tasks, and non-wonky tasks\n",
    "# This helps identify temporal patterns that distinguish wonky from non-wonky behavior\n",
    "\n",
    "temporal_features = ['is_weekend', 'is_night', 'is_business_hour', \n",
    "                     'is_business_hour_weekday', 'is_business_hour_weekend']\n",
    "\n",
    "# Ensure we have wonky_study_count column for grouping\n",
    "if 'wonky_study_count' not in user_info_df.columns:\n",
    "    # Merge wonky counts if needed\n",
    "    if 'respondentPk' in user_info_df.columns and 'respondentPk' in wonky_studies_df.columns:\n",
    "        wonky_mapping = wonky_studies_df.set_index('respondentPk')['wonky_task_instances'].to_dict()\n",
    "        user_info_df['wonky_study_count'] = user_info_df['respondentPk'].map(wonky_mapping).fillna(0)\n",
    "\n",
    "# Print formatted breakdown\n",
    "print(create_temporal_breakdown_summary(\n",
    "    user_info_df,\n",
    "    temporal_features=temporal_features,\n",
    "    group_col='wonky_study_count',\n",
    "    group_threshold=0\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-Squared Tests for Temporal Features\n",
    "\n",
    "Testing independence between temporal features and wonky study participation.\n",
    "Chi-squared test determines if temporal patterns differ significantly between wonky and non-wonky groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform chi-squared tests for temporal features\n",
    "# Why: Tests whether temporal features are independent of wonky study participation\n",
    "# Significant results indicate temporal patterns that distinguish wonky from non-wonky behavior\n",
    "\n",
    "chi_square_results = perform_chi_square_tests(\n",
    "    user_info_df,\n",
    "    temporal_features=temporal_features,\n",
    "    group_var='wonky_study_count',\n",
    "    significance_level=0.01\n",
    ")\n",
    "\n",
    "print(\"Chi-Squared Test Results:\")\n",
    "print(\"=\" * 80)\n",
    "display(chi_square_results)\n",
    "\n",
    "# Summary\n",
    "if len(chi_square_results) > 0:\n",
    "    significant_features = chi_square_results[chi_square_results['significant'] == True]\n",
    "    print(f\"\\nSignificant temporal features (p < 0.01): {len(significant_features)}\")\n",
    "    if len(significant_features) > 0:\n",
    "        print(\"Features:\", ', '.join(significant_features.index.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize chi-squared statistics\n",
    "# Why: Bar chart makes it easy to see which temporal features show strongest associations with wonky studies\n",
    "# Higher chi-squared values indicate stronger associations\n",
    "\n",
    "if len(chi_square_results) > 0:\n",
    "    fig_chi2 = create_chi_squared_bar_chart(\n",
    "        chi_square_results,\n",
    "        chi2_col='chi2',\n",
    "        p_value_col='chi_p_value',\n",
    "        significance_level=0.01,\n",
    "        title=\"Chi-Squared Statistic by Temporal Feature\"\n",
    "    )\n",
    "    fig_chi2.show()\n",
    "else:\n",
    "    print(\"No chi-squared test results available for visualization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6d4ca7c-c549-4c81-817a-d048aa8ebd70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create task speed features using modular function\n",
    "user_info_df = create_task_speed_features(\n",
    "    user_info_df,\n",
    "    task_time_col=\"task_time_taken_s\",\n",
    "    suspicious_threshold=feature_config['time_thresholds']['suspicious_fast_seconds'],\n",
    "    very_fast_threshold=feature_config['time_thresholds']['very_fast_seconds'],\n",
    "    very_slow_threshold=feature_config['time_thresholds']['very_slow_minutes'] * 60\n",
    ")\n",
    "\n",
    "print(f\"Suspiciously fast (<{feature_config['time_thresholds']['suspicious_fast_seconds']}s): {user_info_df['is_suspiciously_fast'].sum():,} ({user_info_df['is_suspiciously_fast'].mean()*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e81c78c-3e3e-463e-9615-2e5f83d5b8e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create respondent-level behavioral features\n",
    "respondent_features = create_respondent_behavioral_features(\n",
    "    user_info_df,\n",
    "    respondent_id_col=\"respondentPk\",\n",
    "    date_col=\"date_completed\",\n",
    "    config={\n",
    "        'high_volume_percentile': feature_config['volume_thresholds']['high_volume_percentile'],\n",
    "        'extreme_volume_percentile': feature_config['volume_thresholds']['extreme_volume_percentile'],\n",
    "        'velocity_bins': feature_config['velocity_bins'],\n",
    "        'velocity_labels': feature_config['velocity_labels']\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Aggregated to {respondent_features.shape[0]:,} respondents\")\n",
    "print(f\"Avg tasks per respondent: {respondent_features['total_tasks'].mean():.2f}\")\n",
    "print(f\"Avg suspicious fast rate: {respondent_features['suspicious_fast_rate'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb0bec3-ec25-4319-b561-151883ca5ee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add wonky features using modular function\n",
    "respondent_features = add_wonky_features(\n",
    "    respondent_features,\n",
    "    wonky_studies_df,\n",
    "    respondent_id_col=\"respondentPk\"\n",
    ")\n",
    "\n",
    "print(f\"Wonky features added\")\n",
    "print(f\"Respondents with wonky tasks: {(respondent_features['wonky_task_ratio'] > 0).sum():,}\")\n",
    "print(f\"High wonky concentration (>50%): {respondent_features['is_high_wonky'].sum():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Test Results Visualization\n",
    "\n",
    "Visualizing statistical test results to identify features with strongest discrimination between wonky and non-wonky groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dual-axis chart showing count differences and Welch's t-statistic\n",
    "# Why: Combines effect size (count differences) with statistical significance (t-statistic)\n",
    "# Helps identify features that are both statistically significant and practically meaningful\n",
    "\n",
    "# Prepare data for dual-axis chart\n",
    "if len(statistical_results) > 0 and 'welch_statistic' in statistical_results.columns:\n",
    "    # Calculate normalized count differences\n",
    "    testdf_2 = statistical_results.copy()\n",
    "    testdf_2['count_difference'] = testdf_2['wonky_mean'] - testdf_2['non_wonky_mean']\n",
    "    testdf_2['count_difference_nrm'] = testdf_2['count_difference'] / (testdf_2['non_wonky_mean'].replace(0, np.nan))\n",
    "    \n",
    "    # Set index to metric name for plotting\n",
    "    testdf_2 = testdf_2.set_index('metric')\n",
    "    \n",
    "    # Create dual-axis chart\n",
    "    fig_dual = create_dual_axis_statistical_chart(\n",
    "        testdf_2,\n",
    "        count_diff_col='count_difference_nrm',\n",
    "        t_stat_col='welch_statistic',\n",
    "        title=\"Ave Wonky Count Difference & Welch's t-statistic by Feature\"\n",
    "    )\n",
    "    fig_dual.show()\n",
    "else:\n",
    "    print(\"Statistical results not available for dual-axis chart\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Comparisons\n",
    "\n",
    "Comparing distributions of key features between wonky and non-wonky groups to visualize differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution comparisons for key features\n",
    "# Why: Visual comparison helps understand the shape and spread of differences between groups\n",
    "# Histograms show full distributions, box plots show quartiles and outliers\n",
    "\n",
    "key_features_for_dist = ['total_tasks', 'suspicious_fast_rate', 'days_active', \n",
    "                         'avg_task_time', 'wonky_task_ratio']\n",
    "\n",
    "for feature in key_features_for_dist:\n",
    "    if feature in respondent_features.columns:\n",
    "        # Create histogram comparison\n",
    "        fig_dist = create_distribution_comparison(\n",
    "            respondent_features,\n",
    "            feature=feature,\n",
    "            group_col='has_wonky_tasks',\n",
    "            group1_value=1,\n",
    "            group2_value=0,\n",
    "            plot_type='histogram',\n",
    "            group1_name='Wonky Users',\n",
    "            group2_name='Non-Wonky Users',\n",
    "            title=f'{feature.replace(\"_\", \" \").title()} Distribution: Wonky vs Non-Wonky Users'\n",
    "        )\n",
    "        fig_dist.show()\n",
    "        \n",
    "        # Create box plot comparison\n",
    "        fig_box = create_distribution_comparison(\n",
    "            respondent_features,\n",
    "            feature=feature,\n",
    "            group_col='has_wonky_tasks',\n",
    "            group1_value=1,\n",
    "            group2_value=0,\n",
    "            plot_type='box',\n",
    "            group1_name='Wonky Users',\n",
    "            group2_name='Non-Wonky Users',\n",
    "            title=f'{feature.replace(\"_\", \" \").title()} Distribution: Wonky vs Non-Wonky Users'\n",
    "        )\n",
    "        fig_box.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature summary tables\n",
    "# Why: Provides detailed statistics (mean, median, std, count) for wonky vs non-wonky groups\n",
    "# Helps quantify differences identified in visualizations\n",
    "\n",
    "feature_breakdown = create_feature_breakdown_table(\n",
    "    respondent_features,\n",
    "    feature_col='has_wonky_tasks',\n",
    "    group_col='has_wonky_tasks',\n",
    "    group1_value=1,\n",
    "    group2_value=0,\n",
    "    metrics=key_features_for_dist\n",
    ")\n",
    "\n",
    "print(\"Feature Breakdown: Wonky vs Non-Wonky Users\")\n",
    "print(\"=\" * 100)\n",
    "display(feature_breakdown)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic Group Comparisons\n",
    "\n",
    "Comparing wonky task rates across demographic groups (platforms, hardware versions, locales) using statistical tests.\n",
    "This helps identify if certain demographics are more associated with wonky study participation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare wonky task rates across demographic groups\n",
    "# Why: Identifies if certain platforms, hardware versions, or locales show different wonky task patterns\n",
    "# Uses Mann-Whitney U and Welch's t-test to compare each pair of demographic groups\n",
    "\n",
    "demographic_cols = ['platform_name', 'hardware_version', 'survey_locale']\n",
    "\n",
    "demographic_results = {}\n",
    "\n",
    "for demo_col in demographic_cols:\n",
    "    if demo_col in respondent_features.columns:\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"Demographic Comparison: {demo_col}\")\n",
    "        print('=' * 80)\n",
    "        \n",
    "        # Perform comparisons\n",
    "        demo_comparisons = compare_demographic_groups(\n",
    "            respondent_features,\n",
    "            demographic_col=demo_col,\n",
    "            target_col='wonky_task_ratio',\n",
    "            min_group_size=10,\n",
    "            significance_level=0.05\n",
    "        )\n",
    "        \n",
    "        if len(demo_comparisons) > 0:\n",
    "            demographic_results[demo_col] = demo_comparisons\n",
    "            \n",
    "            # Show summary statistics by group\n",
    "            group_summary = respondent_features.groupby(demo_col).agg({\n",
    "                'wonky_task_ratio': ['mean', 'median', 'count'],\n",
    "                'has_wonky_tasks': 'sum'\n",
    "            }).round(4)\n",
    "            group_summary.columns = ['mean_wonky_ratio', 'median_wonky_ratio', 'total_count', 'wonky_count']\n",
    "            group_summary['wonky_rate'] = (group_summary['wonky_count'] / group_summary['total_count'] * 100).round(2)\n",
    "            group_summary = group_summary.sort_values('mean_wonky_ratio', ascending=False)\n",
    "            \n",
    "            print(f\"\\nSummary by {demo_col}:\")\n",
    "            display(group_summary)\n",
    "            \n",
    "            # Show significant comparisons\n",
    "            significant_comps = demo_comparisons[demo_comparisons['mw_significant'] == True]\n",
    "            if len(significant_comps) > 0:\n",
    "                print(f\"\\nSignificant differences (p < 0.05): {len(significant_comps)}\")\n",
    "                display(significant_comps[['group1', 'group2', 'mean_difference', 'mw_p_value', 'welch_p_value', 'tests_agree']])\n",
    "            else:\n",
    "                print(\"\\nNo significant differences found between groups\")\n",
    "        else:\n",
    "            print(f\"Insufficient data for {demo_col} comparisons\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize demographic comparisons\n",
    "# Why: Bar charts make it easy to see which demographic groups have higher wonky task rates\n",
    "# Helps identify patterns that may inform modeling or business decisions\n",
    "\n",
    "for demo_col in demographic_cols:\n",
    "    if demo_col in respondent_features.columns and demo_col in demographic_results:\n",
    "        # Create summary by group\n",
    "        demo_summary = respondent_features.groupby(demo_col).agg({\n",
    "            'wonky_task_ratio': 'mean',\n",
    "            'has_wonky_tasks': ['sum', 'count']\n",
    "        }).reset_index()\n",
    "        demo_summary.columns = [demo_col, 'mean_wonky_ratio', 'wonky_count', 'total_count']\n",
    "        demo_summary['wonky_rate'] = (demo_summary['wonky_count'] / demo_summary['total_count'] * 100)\n",
    "        demo_summary = demo_summary.sort_values('mean_wonky_ratio', ascending=False)\n",
    "        \n",
    "        # Bar chart of wonky rates by demographic group\n",
    "        fig_demo = create_bar_plot(\n",
    "            demo_summary,\n",
    "            x=demo_col,\n",
    "            y='wonky_rate',\n",
    "            title=f'Wonky Task Rate by {demo_col.replace(\"_\", \" \").title()}',\n",
    "            labels={demo_col: demo_col.replace(\"_\", \" \").title(), 'wonky_rate': 'Wonky Task Rate (%)'},\n",
    "            color='wonky_rate',\n",
    "            color_continuous_scale='Reds',\n",
    "            text='wonky_rate',\n",
    "            texttemplate='%{text:.1f}%',\n",
    "            textposition='outside',\n",
    "            tickangle=45\n",
    "        )\n",
    "        fig_demo.show()\n",
    "        \n",
    "        # Box plot comparing wonky task ratios across groups\n",
    "        fig_demo_box = create_box_plot(\n",
    "            respondent_features,\n",
    "            x=demo_col,\n",
    "            y='wonky_task_ratio',\n",
    "            title=f'Wonky Task Ratio Distribution by {demo_col.replace(\"_\", \" \").title()}',\n",
    "            labels={demo_col: demo_col.replace(\"_\", \" \").title(), 'wonky_task_ratio': 'Wonky Task Ratio'},\n",
    "            tickangle=45\n",
    "        )\n",
    "        fig_demo_box.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection Guidance\n",
    "\n",
    "Based on EDA results, identify which features show strongest discrimination and should be prioritized for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance summary\n",
    "# Why: Ranks features by statistical significance, effect size, and test agreement\n",
    "# Helps prioritize which features to include in models\n",
    "\n",
    "if len(statistical_results) > 0:\n",
    "    # Create feature ranking\n",
    "    feature_ranking = statistical_results.copy()\n",
    "    \n",
    "    # Calculate composite score (combining significance and effect size)\n",
    "    if 'welch_statistic' in feature_ranking.columns:\n",
    "        # Use absolute t-statistic as effect size indicator\n",
    "        feature_ranking['effect_size'] = feature_ranking['welch_statistic'].abs()\n",
    "    else:\n",
    "        feature_ranking['effect_size'] = feature_ranking['mean_difference'].abs()\n",
    "    \n",
    "    # Create ranking score (lower p-value + higher effect size = better)\n",
    "    feature_ranking['ranking_score'] = (\n",
    "        (1 - feature_ranking['mw_p_value'].clip(0, 1)) * 0.5 +  # Significance component\n",
    "        (feature_ranking['effect_size'] / feature_ranking['effect_size'].max()) * 0.5  # Effect size component\n",
    "    )\n",
    "    \n",
    "    # Add test agreement indicator\n",
    "    if 'tests_agree' in feature_ranking.columns:\n",
    "        feature_ranking['both_tests_agree'] = feature_ranking['tests_agree']\n",
    "    else:\n",
    "        feature_ranking['both_tests_agree'] = True\n",
    "    \n",
    "    # Sort by ranking score\n",
    "    feature_ranking = feature_ranking.sort_values('ranking_score', ascending=False)\n",
    "    \n",
    "    # Select key columns for display\n",
    "    display_cols = ['metric', 'mean_difference', 'mw_p_value', 'welch_p_value']\n",
    "    if 'welch_statistic' in feature_ranking.columns:\n",
    "        display_cols.append('welch_statistic')\n",
    "    if 'both_tests_agree' in feature_ranking.columns:\n",
    "        display_cols.append('both_tests_agree')\n",
    "    display_cols.append('ranking_score')\n",
    "    \n",
    "    available_cols = [col for col in display_cols if col in feature_ranking.columns]\n",
    "    \n",
    "    print(\"Feature Ranking for Modeling (Top Features)\")\n",
    "    print(\"=\" * 100)\n",
    "    print(\"Ranked by: Statistical significance + Effect size\")\n",
    "    print(\"Higher ranking_score = better feature for modeling\")\n",
    "    print()\n",
    "    display(feature_ranking[available_cols].head(20))\n",
    "    \n",
    "    # Identify top features\n",
    "    top_features = feature_ranking.head(10)['metric'].tolist()\n",
    "    print(f\"\\nTop 10 Features Recommended for Modeling:\")\n",
    "    for i, feat in enumerate(top_features, 1):\n",
    "        print(f\"  {i}. {feat}\")\n",
    "else:\n",
    "    print(\"Statistical results not available for feature ranking\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling Recommendations\n",
    "\n",
    "Based on EDA findings, recommendations for feature selection and modeling approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate modeling recommendations\n",
    "# Why: Synthesizes EDA findings into actionable recommendations for model development\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Check statistical test results\n",
    "if len(statistical_results) > 0:\n",
    "    significant_features = statistical_results[\n",
    "        (statistical_results['mw_significant'] == True) & \n",
    "        (statistical_results.get('welch_significant', pd.Series([True] * len(statistical_results))) == True)\n",
    "    ]\n",
    "    \n",
    "    if len(significant_features) > 0:\n",
    "        recommendations.append(f\"✓ {len(significant_features)} features show significant differences (both tests agree)\")\n",
    "        recommendations.append(f\"  → Prioritize these features: {', '.join(significant_features['metric'].head(5).tolist())}\")\n",
    "    \n",
    "    # Check effect sizes\n",
    "    if 'mean_difference' in statistical_results.columns:\n",
    "        large_effect = statistical_results[statistical_results['mean_difference'].abs() > statistical_results['mean_difference'].abs().quantile(0.75)]\n",
    "        if len(large_effect) > 0:\n",
    "            recommendations.append(f\"✓ {len(large_effect)} features show large effect sizes\")\n",
    "            recommendations.append(f\"  → These features have practical significance: {', '.join(large_effect['metric'].head(3).tolist())}\")\n",
    "\n",
    "# Check temporal features\n",
    "if len(chi_square_results) > 0:\n",
    "    sig_temporal = chi_square_results[chi_square_results['significant'] == True]\n",
    "    if len(sig_temporal) > 0:\n",
    "        recommendations.append(f\"✓ {len(sig_temporal)} temporal features are significantly associated with wonky studies\")\n",
    "        recommendations.append(f\"  → Include temporal features: {', '.join(sig_temporal.index.tolist())}\")\n",
    "\n",
    "# Check demographic differences\n",
    "if demographic_results:\n",
    "    total_sig_demo = sum(len(df[df['mw_significant'] == True]) for df in demographic_results.values())\n",
    "    if total_sig_demo > 0:\n",
    "        recommendations.append(f\"✓ Found {total_sig_demo} significant demographic group differences\")\n",
    "        recommendations.append(f\"  → Consider including demographic features as interaction terms\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODELING RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "for rec in recommendations:\n",
    "    print(rec)\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. Use top-ranked features from feature ranking for initial model\")\n",
    "print(\"2. Include temporal features that show significant associations\")\n",
    "print(\"3. Consider demographic interactions if significant differences found\")\n",
    "print(\"4. Validate feature importance using model-based methods (e.g., Random Forest feature importance)\")\n",
    "print(\"5. Monitor model performance on features identified as significant in EDA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "435a6a6b-7164-41a0-9085-c9d74afcb688",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create fraud risk score using modular function\n",
    "respondent_features = create_fraud_risk_score(\n",
    "    respondent_features,\n",
    "    config={\n",
    "        'fraud_score_weights': feature_config['fraud_score_weights'],\n",
    "        'fraud_score_thresholds': feature_config['fraud_score_thresholds'],\n",
    "        'fraud_score_bins': feature_config['fraud_score_bins'],\n",
    "        'fraud_score_labels': feature_config['fraud_score_labels'],\n",
    "        'suspected_fraud_threshold': feature_config['suspected_fraud_threshold']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create wonky risk score\n",
    "respondent_features = create_wonky_risk_score(\n",
    "    respondent_features,\n",
    "    config={\n",
    "        'wonky_score_weights': feature_config['wonky_score_weights'],\n",
    "        'wonky_score_thresholds': feature_config['wonky_score_thresholds'],\n",
    "        'wonky_score_bins': feature_config['wonky_score_bins'],\n",
    "        'wonky_score_labels': feature_config['wonky_score_labels']\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Fraud risk distribution:\")\n",
    "print(respondent_features['fraud_risk_tier'].value_counts().sort_index())\n",
    "print(f\"Suspected fraud rate: {respondent_features['suspected_fraud'].mean()*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4deb5513-3592-44aa-8daf-2efb2d88965b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Hypothesis Testing\n",
    "\n",
    "Running Mann-Whitney U test and Welch's t-test for validation.\n",
    "\n",
    "Check if Mann-Whitney U and Welch's t-test agree on significance.\n",
    "When both tests agree, we have higher confidence in the results.\n",
    "When they disagree, it signals need for further investigation.\n",
    "\n",
    "Compare wonky vs non-wonky users using statistical tests.\n",
    "- Mann-Whitney U test (non-parametric, primary test)\n",
    "- Welch's t-test (parametric validator/sense check)\n",
    "Uses `compare_groups_with_both_tests()` to run both tests simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aab5bd3c-cefe-4d99-94db-16cae34cd01c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create binary flag for wonky users\n",
    "respondent_features['has_wonky_tasks'] = (respondent_features['wonky_task_ratio'] > 0).astype(int)\n",
    "\n",
    "print(f\"Wonky users: {respondent_features['has_wonky_tasks'].sum():,} ({respondent_features['has_wonky_tasks'].mean()*100:.2f}%)\")\n",
    "print(f\"Non-wonky users: {(respondent_features['has_wonky_tasks'] == 0).sum():,} ({(respondent_features['has_wonky_tasks'] == 0).mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d208e815-db6e-4709-8182-7b42ad5af3dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform statistical tests using modular function\n",
    "statistical_results = compare_groups_with_both_tests(\n",
    "    respondent_features,\n",
    "    group_col=stats_config['group_comparison']['group_col'],\n",
    "    metrics=stats_config['test_metrics'],\n",
    "    group1_value=stats_config['group_comparison']['group1_value'],\n",
    "    group2_value=stats_config['group_comparison']['group2_value'],\n",
    "    significance_level=stats_config['significance_level']\n",
    ")\n",
    "\n",
    "print(\"Statistical Test Results (Mann-Whitney U + Welch's t-test):\")\n",
    "print(\"=\" * 100)\n",
    "display(statistical_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1ad1333-68b3-42f9-9c7f-5434af9e2c3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze test agreement\n",
    "if 'tests_agree' in statistical_results.columns:\n",
    "    agreement_stats = statistical_results['tests_agree'].value_counts()\n",
    "    print(\"Test Agreement Analysis:\")\n",
    "    print(f\"Metrics where tests agree: {agreement_stats.get(True, 0)}\")\n",
    "    print(f\"Metrics where tests disagree: {agreement_stats.get(False, 0)}\")\n",
    "    \n",
    "    # Show metrics where tests disagree\n",
    "    disagree = statistical_results[statistical_results['tests_agree'] == False]\n",
    "    if len(disagree) > 0:\n",
    "        print(\"\\nMetrics where tests disagree (need investigation):\")\n",
    "        display(disagree[['metric', 'mw_p_value', 'welch_p_value', 'mw_significant', 'welch_significant']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efdf385d-5be8-4899-ab0f-f0fd82844641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze thresholds for key features\n",
    "print(\"Threshold Analysis: Task Volume\")\n",
    "print(\"=\" * 80)\n",
    "volume_thresholds = analyze_thresholds(\n",
    "    respondent_features,\n",
    "    feature='total_tasks',\n",
    "    bins=stats_config['threshold_analysis']['default_bins'],\n",
    "    target_col='has_wonky_tasks'\n",
    ")\n",
    "display(volume_thresholds)\n",
    "\n",
    "print(\"\\nThreshold Analysis: Suspicious Fast Rate\")\n",
    "print(\"=\" * 80)\n",
    "speed_thresholds = analyze_thresholds(\n",
    "    respondent_features,\n",
    "    feature='suspicious_fast_rate',\n",
    "    bins=stats_config['threshold_analysis']['default_bins'],\n",
    "    target_col='has_wonky_tasks'\n",
    ")\n",
    "display(speed_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b74a098-f760-43ae-9169-7bb54e2f3216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example visualizations using modular functions\n",
    "\n",
    "# Histogram of fraud risk score\n",
    "fig_fraud_risk = create_histogram(\n",
    "    respondent_features,\n",
    "    x='fraud_risk_score',\n",
    "    color='fraud_risk_tier',\n",
    "    nbins=11,\n",
    "    title='Fraud Risk Score Distribution',\n",
    "    labels={'fraud_risk_score': 'Fraud Risk Score (0-10)'}\n",
    ")\n",
    "fig_fraud_risk.show()\n",
    "\n",
    "# Box plot comparing wonky vs non-wonky users\n",
    "fig_speed = create_box_plot(\n",
    "    respondent_features,\n",
    "    x='has_wonky_tasks',\n",
    "    y='avg_task_time',\n",
    "    color='has_wonky_tasks',\n",
    "    title='Average Task Time: Wonky vs Non-Wonky Users',\n",
    "    labels={'has_wonky_tasks': 'Has Wonky Tasks (1=Yes, 0=No)', 'avg_task_time': 'Average Task Time (seconds)'},\n",
    "    category_orders={'has_wonky_tasks': [0, 1]},\n",
    "    boxmean='sd'\n",
    ")\n",
    "fig_speed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fff57faa-c32f-4ce9-a388-d54e04f07247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "**Key Findings:**\n",
    "- Statistical tests (Mann-Whitney U + Welch's t-test) identify significant differences between wonky and non-wonky users\n",
    "- Temporal features show strong associations with wonky study participation (chi-squared tests)\n",
    "- Feature engineering creates actionable behavioral indicators (task speed, volume, velocity)\n",
    "- Demographic groups (platforms, hardware, locales) show varying wonky task rates\n",
    "- Both statistical tests provide validation - agreement increases confidence in findings\n",
    "- Visualizations highlight features with strongest discrimination power\n",
    "\n",
    "**Feature Selection Insights:**\n",
    "- Top-ranked features combine statistical significance with practical effect sizes\n",
    "- Temporal patterns (business hours, night tasks) are strong discriminators\n",
    "- Behavioral features (task volume, speed) show consistent differences\n",
    "- Demographic features may be useful as interaction terms\n",
    "\n",
    "**Next Steps:**\n",
    "- Use top-ranked features from feature ranking for model training\n",
    "- Include temporal features that show significant chi-squared associations\n",
    "- Consider demographic interactions based on group comparison results\n",
    "- Apply fraud risk thresholds to filter suspicious users\n",
    "- Build models using features identified as significant in EDA\n",
    "- Validate feature importance using model-based methods (Random Forest feature importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec763b35-0e7f-4c22-99b0-30642a498194",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "resoondent_eda_refactored",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
