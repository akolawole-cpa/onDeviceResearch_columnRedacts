{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "208d2215-c24b-4a29-a832-53969d83cc23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import yaml\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"PyYAML is not installed. Please run the previous cell to install it, \"\n",
    "        \"or run: %pip install pyyaml>=6.0\"\n",
    "    )\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from eda.feature_engineering import (\n",
    "    create_time_features,\n",
    "    create_task_speed_features,\n",
    "    create_respondent_behavioral_features,\n",
    "    add_wonky_features,\n",
    "    create_fraud_risk_score,\n",
    "    create_wonky_risk_score\n",
    ")\n",
    "from eda.statistical_tests import (\n",
    "    compare_groups_statistically,\n",
    "    compare_groups_with_both_tests,\n",
    "    analyze_thresholds,\n",
    "    perform_chi_square_tests,\n",
    "    perform_mannwhitney_tests,\n",
    "    perform_welch_ttests,\n",
    "    perform_two_proportion_z_tests,\n",
    "    compare_demographic_groups\n",
    ")\n",
    "\n",
    "from eda.visualizations import (\n",
    "    create_histogram,\n",
    "    create_box_plot,\n",
    "    create_scatter_plot,\n",
    "    create_bar_plot,\n",
    "    create_temporal_breakdown_summary,\n",
    "    create_task_speed_breakdown_summary,\n",
    "    create_chi_squared_bar_chart,\n",
    "    create_dual_axis_statistical_chart,\n",
    "    create_feature_breakdown_table,\n",
    "    create_distribution_comparison,\n",
    "    calculate_temporal_feature_deltas,       \n",
    "    create_chi_squared_delta_dual_axis_chart,\n",
    ")\n",
    "\n",
    "# Load configs\n",
    "with open('../configs/feature_engineering.yaml', 'r') as f:\n",
    "    feature_config = yaml.safe_load(f)\n",
    "\n",
    "with open('../configs/statistical_tests.yaml', 'r') as f:\n",
    "    stats_config = yaml.safe_load(f)\n",
    "\n",
    "with open('../configs/data_paths.yaml', 'r') as f:\n",
    "    paths_config = yaml.safe_load(f)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"âœ“ Imports and configs loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72a8e1ea-438f-4c00-87a3-340c6fe9e07c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "notebook_path = os.getcwd() \n",
    "repo_root = os.path.abspath(os.path.join(notebook_path, \"..\"))\n",
    "misc_dir = os.path.join(repo_root, \"misc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2efa83ef-8fa8-4774-b46a-0c4a274aab4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "respondent_features_path = os.path.join(misc_dir,\n",
    "                          os.path.basename(paths_config['output_files'].get('respondent_features')))\n",
    "\n",
    "df = pd.read_parquet(respondent_features_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f7609c5-3fc4-490e-af21-0631660c6642",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78b78686-506f-4335-9fe5-f6eb551ae248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def train_rf_and_plot_importance(df, target_col='wonky_study_count', top_n=20):\n",
    "    \"\"\"\n",
    "    Trains a Random Forest to predict wonkiness and plots feature importance.\n",
    "    \n",
    "    Args:\n",
    "        df: Respondent-level dataframe\n",
    "        target_col: Target variable (e.g., wonky_study_count or binary flag)\n",
    "        top_n: Number of top features to display\n",
    "        \n",
    "    Returns:\n",
    "        model: Trained RF model\n",
    "        feature_importance_df: Dataframe of features ranked by importance\n",
    "    \"\"\"\n",
    "    # 1. Prepare Data\n",
    "    # Convert target to binary for clearer classification signals (Wonky vs Not)\n",
    "    # OR keep as regression if you want to predict magnitude. Classification is safer for \"blocking\".\n",
    "    y = (df[target_col] > 0).astype(int)\n",
    "    \n",
    "    # Drop non-feature columns (IDs, raw dates, the target itself)\n",
    "    # Adjust this exclusion list based on your actual column names\n",
    "    exclude_cols = ['respondentPk', 'respondent_pk', target_col, 'wonky_task_ratio', \n",
    "                    'wonky_task_instances', 'wonky_study_flag', 'total_wonky_studies']\n",
    "    \n",
    "    X = df.drop(columns=[c for c in exclude_cols if c in df.columns])\n",
    "    \n",
    "    # Handle categorical columns (One-Hot Encoding if not already done)\n",
    "    # Assuming 'respondent_features' is largely numeric/aggregated already.\n",
    "    # If there are string columns, we get_dummies them.\n",
    "    X = pd.get_dummies(X, dummy_na=True)\n",
    "    X = X.fillna(0) # Simple imputation for RF\n",
    "    \n",
    "    # 2. Train Random Forest\n",
    "    # Using balanced class_weight to handle the rarity of wonky users\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=None, # Let trees grow to find interactions\n",
    "        min_samples_leaf=5, # Prevent overfitting to single users\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        oob_score=True\n",
    "    )\n",
    "    \n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    # 3. Extract Feature Importance (Gini Importance)\n",
    "    importances = rf.feature_importances_\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    feat_imp_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # 4. Print Model Quality Metrics\n",
    "    print(f\"Random Forest OOB Score (Accuracy): {rf.oob_score_:.4f}\")\n",
    "    \n",
    "    # 5. Output for User\n",
    "    return rf, feat_imp_df.head(top_n)\n",
    "\n",
    "# Example usage (commented out until dataframe is loaded):\n",
    "# rf_model, top_features = train_rf_and_plot_importance(respondent_features)\n",
    "# print(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eb1fe4d-f4dc-4846-b3c8-5381d871c319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "514987a9-f0e1-4f09-beb0-0b2e01f92b68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare Target\n",
    "y = (df['wonky_study_count'] > 0).astype(int)\n",
    "\n",
    "# Drop ALL non-feature columns (Dates, IDs, Targets)\n",
    "cols_to_drop = [\n",
    "    'respondentPk',              # <--- THIS WAS THE CULPRIT\n",
    "    'first_task_date', \n",
    "    'last_task_date',\n",
    "    'wonky_study_count', \n",
    "    'wonky_task_ratio', \n",
    "    'is_high_wonky', \n",
    "    'is_quite_wonky'\n",
    "]\n",
    "\n",
    "X = df.drop(columns=[c for c in cols_to_drop if c in df.columns])\n",
    "\n",
    "# One-Hot Encode remaining categories\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "X = X.fillna(0)\n",
    "\n",
    "# Train\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    class_weight='balanced', \n",
    "    random_state=42, \n",
    "    n_jobs=-1, \n",
    "    oob_score=True,\n",
    "    max_depth=15  # Limit tree depth to prevent memorization\n",
    ")\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Show Importance\n",
    "imp = pd.DataFrame({\n",
    "    'feature': X.columns, \n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(imp.head(20))\n",
    "print(f\"OOB Accuracy: {rf.oob_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdfd1aea-75e5-40ea-b0a2-341f6eec658f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_rf_and_plot_importance(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e17dd9d-f550-4689-90d7-85906d01971c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "respondent_modelling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
